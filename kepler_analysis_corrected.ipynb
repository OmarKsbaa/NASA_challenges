{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c4ec74e",
   "metadata": {},
   "source": [
    "# 🛰️ Kepler Mission - Exoplanet Classification Analysis\n",
    "## NASA Space Apps Challenge - Complete 3-Class Implementation\n",
    "\n",
    "### 🎯 **Objective**\n",
    "Classify Kepler Objects of Interest (KOIs) into three categories using the complete Exoplanet Archive disposition:\n",
    "- **CONFIRMED**: Validated exoplanets (2,746 samples)\n",
    "- **CANDIDATE**: Objects awaiting validation (1,979 samples)  \n",
    "- **FALSE POSITIVE**: Ruled out objects (4,839 samples)\n",
    "\n",
    "### 📊 **Dataset Information**\n",
    "- **Source**: NASA Exoplanet Archive - Kepler Cumulative Table\n",
    "- **File**: `cumulative_2025.09.25_10.52.58.csv`\n",
    "- **Total Samples**: 9,564 Kepler Objects of Interest\n",
    "- **Target Variable**: `koi_disposition` (3-class classification)\n",
    "- **Mission**: Kepler Space Telescope (2009-2017)\n",
    "\n",
    "### 🔬 **Analysis Approach**\n",
    "1. **Data Loading & Exploration**: Comprehensive EDA with target analysis\n",
    "2. **Feature Engineering**: Astronomical feature derivation and preprocessing\n",
    "3. **Model Training**: 5-model comparison with XGBoost optimization\n",
    "4. **Evaluation**: Cross-validation, feature importance, and performance analysis\n",
    "5. **Results**: Integration-ready outputs for multi-dataset comparison\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cf63c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import xgboost as xgb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"🚀 Kepler Analysis - Libraries Imported Successfully!\")\n",
    "print(\"📅 Analysis Date: September 26, 2025\")\n",
    "print(\"🎯 Target: koi_disposition (3-class classification)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36aa053a",
   "metadata": {},
   "source": [
    "## 📂 Data Loading and Initial Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f477b4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the Kepler dataset\n",
    "print(\"📂 Loading Kepler Dataset...\")\n",
    "kepler_data = pd.read_csv('cumulative_2025.09.25_10.52.58.csv', comment='#', low_memory=False)\n",
    "\n",
    "print(f\"✅ Dataset loaded successfully!\")\n",
    "print(f\"📊 Dataset shape: {kepler_data.shape}\")\n",
    "print(f\"📋 Columns: {kepler_data.shape[1]}\")\n",
    "print(f\"📈 Rows: {kepler_data.shape[0]:,}\")\n",
    "\n",
    "# Display basic information\n",
    "print(f\"\\n🔍 Dataset Overview:\")\n",
    "print(f\"  • Memory usage: {kepler_data.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "print(f\"  • Data types: {kepler_data.dtypes.value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4036ffda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze target variable - koi_disposition (the correct 3-class target)\n",
    "print(\"🎯 Target Variable Analysis: 'koi_disposition'\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "target_counts = kepler_data['koi_disposition'].value_counts()\n",
    "target_pct = kepler_data['koi_disposition'].value_counts(normalize=True) * 100\n",
    "\n",
    "print(\"📊 Class Distribution:\")\n",
    "for class_name, count in target_counts.items():\n",
    "    percentage = target_pct[class_name]\n",
    "    print(f\"  • {class_name}: {count:,} samples ({percentage:.1f}%)\")\n",
    "\n",
    "print(f\"\\n📈 Total samples: {target_counts.sum():,}\")\n",
    "\n",
    "# Verify we have all three classes\n",
    "print(f\"\\n✅ Confirmed 3-class problem:\")\n",
    "print(f\"  • CONFIRMED class: {target_counts.get('CONFIRMED', 0):,} samples\")\n",
    "print(f\"  • CANDIDATE class: {target_counts.get('CANDIDATE', 0):,} samples\") \n",
    "print(f\"  • FALSE POSITIVE class: {target_counts.get('FALSE POSITIVE', 0):,} samples\")\n",
    "\n",
    "# Compare with the binary version (koi_pdisposition)\n",
    "print(f\"\\n🔄 Comparison with koi_pdisposition (binary version):\")\n",
    "binary_counts = kepler_data['koi_pdisposition'].value_counts()\n",
    "print(f\"  • Binary version has {len(binary_counts)} classes: {list(binary_counts.index)}\")\n",
    "print(f\"  • Complete version has {len(target_counts)} classes: {list(target_counts.index)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6da0137f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize target distribution\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "\n",
    "# Plot 1: 3-class target distribution (koi_disposition)\n",
    "target_counts.plot(kind='bar', ax=ax1, color=['#2E86AB', '#A23B72', '#F18F01'], \n",
    "                   edgecolor='black', alpha=0.8)\n",
    "ax1.set_title('Kepler Target Distribution\\n(koi_disposition - 3-class)', \n",
    "              fontsize=14, fontweight='bold')\n",
    "ax1.set_xlabel('Disposition Class')\n",
    "ax1.set_ylabel('Number of Objects')\n",
    "ax1.tick_params(axis='x', rotation=45)\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, v in enumerate(target_counts.values):\n",
    "    ax1.text(i, v + 50, f'{v:,}\\n({target_pct.iloc[i]:.1f}%)', \n",
    "             ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# Plot 2: Comparison pie chart\n",
    "ax2.pie(target_counts.values, labels=target_counts.index, autopct='%1.1f%%',\n",
    "        colors=['#2E86AB', '#A23B72', '#F18F01'], startangle=90)\n",
    "ax2.set_title('Class Distribution\\n(Proportional View)', \n",
    "              fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"📊 Visualization: 3-class target distribution displayed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65ab860",
   "metadata": {},
   "source": [
    "## 🔧 Feature Engineering & Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc81037",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify astronomical features\n",
    "print(\"🔍 Astronomical Feature Identification\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Key astronomical parameters\n",
    "astronomical_keywords = ['koi_', 'pl_', 'st_', 'period', 'radius', 'mass', 'temp', 'mag', 'depth', 'duration', 'snr']\n",
    "astronomical_features = []\n",
    "\n",
    "for col in kepler_data.columns:\n",
    "    col_lower = col.lower()\n",
    "    if any(keyword in col_lower for keyword in astronomical_keywords):\n",
    "        if col not in ['koi_disposition', 'koi_pdisposition']:  # Exclude target columns\n",
    "            astronomical_features.append(col)\n",
    "\n",
    "print(f\"📡 Identified astronomical features ({len(astronomical_features)}):\")\n",
    "for i, feature in enumerate(astronomical_features[:15], 1):\n",
    "    print(f\"  {i:2d}. {feature}\")\n",
    "if len(astronomical_features) > 15:\n",
    "    print(f\"  ... and {len(astronomical_features) - 15} more features\")\n",
    "\n",
    "# Analyze missing data\n",
    "print(f\"\\n🔍 Missing Data Analysis:\")\n",
    "missing_data = kepler_data[astronomical_features].isnull().sum()\n",
    "missing_percent = (missing_data / len(kepler_data)) * 100\n",
    "missing_df = pd.DataFrame({\n",
    "    'Feature': missing_data.index,\n",
    "    'Missing_Count': missing_data.values,\n",
    "    'Missing_Percent': missing_percent.values\n",
    "}).sort_values('Missing_Percent', ascending=False)\n",
    "\n",
    "# Show top 10 features with missing data\n",
    "print(\"📊 Top 10 features with missing data:\")\n",
    "for _, row in missing_df.head(10).iterrows():\n",
    "    if row['Missing_Count'] > 0:\n",
    "        print(f\"  • {row['Feature']}: {row['Missing_Count']:,} ({row['Missing_Percent']:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2f1deae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature preprocessing and engineering\n",
    "print(\"⚙️ Feature Preprocessing Pipeline\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Separate features by type\n",
    "numerical_columns = kepler_data[astronomical_features].select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_columns = kepler_data[astronomical_features].select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "print(f\"📊 Feature breakdown:\")\n",
    "print(f\"  • Numerical features: {len(numerical_columns)}\")\n",
    "print(f\"  • Categorical features: {len(categorical_columns)}\")\n",
    "\n",
    "# Prepare feature matrix X and target vector y\n",
    "X = kepler_data[astronomical_features].copy()\n",
    "y = kepler_data['koi_disposition'].copy()\n",
    "\n",
    "print(f\"\\n🎯 Target preparation:\")\n",
    "print(f\"  • Target variable: koi_disposition\")\n",
    "print(f\"  • Classes: {y.value_counts().to_dict()}\")\n",
    "\n",
    "# Encode target variable\n",
    "target_encoder = LabelEncoder()\n",
    "y_encoded = target_encoder.fit_transform(y.astype(str))\n",
    "target_classes = target_encoder.classes_\n",
    "\n",
    "print(f\"  • Encoded classes: {dict(zip(range(len(target_classes)), target_classes))}\")\n",
    "\n",
    "# Handle numerical features\n",
    "print(f\"\\n🔢 Numerical feature processing:\")\n",
    "numerical_imputer = SimpleImputer(strategy='median')\n",
    "X_numerical = X[numerical_columns].copy()\n",
    "\n",
    "# Replace infinite values\n",
    "X_numerical = X_numerical.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "# Impute missing values\n",
    "X_numerical_imputed = pd.DataFrame(\n",
    "    numerical_imputer.fit_transform(X_numerical),\n",
    "    columns=numerical_columns,\n",
    "    index=X.index\n",
    ")\n",
    "\n",
    "print(f\"  • Features processed: {len(numerical_columns)}\")\n",
    "print(f\"  • Missing values imputed with median\")\n",
    "print(f\"  • Infinite values replaced with NaN then imputed\")\n",
    "\n",
    "# Handle categorical features\n",
    "print(f\"\\n📝 Categorical feature processing:\")\n",
    "if categorical_columns:\n",
    "    categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
    "    label_encoders = {}\n",
    "    \n",
    "    X_categorical = X[categorical_columns].copy()\n",
    "    X_categorical_imputed = pd.DataFrame(\n",
    "        categorical_imputer.fit_transform(X_categorical),\n",
    "        columns=categorical_columns,\n",
    "        index=X.index\n",
    "    )\n",
    "    \n",
    "    # Label encode categorical features\n",
    "    for col in categorical_columns:\n",
    "        le = LabelEncoder()\n",
    "        X_categorical_imputed[col] = le.fit_transform(X_categorical_imputed[col].astype(str))\n",
    "        label_encoders[col] = le\n",
    "    \n",
    "    # Combine numerical and categorical features\n",
    "    X_processed = pd.concat([X_numerical_imputed, X_categorical_imputed], axis=1)\n",
    "    print(f\"  • Categorical features encoded: {len(categorical_columns)}\")\n",
    "else:\n",
    "    X_processed = X_numerical_imputed\n",
    "    label_encoders = {}\n",
    "    print(f\"  • No categorical features found\")\n",
    "\n",
    "print(f\"\\n✅ Preprocessing completed\")\n",
    "print(f\"  • Final feature matrix shape: {X_processed.shape}\")\n",
    "print(f\"  • Target vector shape: {y_encoded.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6128eb15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature scaling\n",
    "print(\"📏 Feature Scaling\")\n",
    "print(\"=\" * 20)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_processed)\n",
    "\n",
    "print(f\"✅ Features scaled using StandardScaler\")\n",
    "print(f\"  • Scaled feature matrix shape: {X_scaled.shape}\")\n",
    "print(f\"  • Features centered (mean ≈ 0) and scaled (std ≈ 1)\")\n",
    "\n",
    "# Create DataFrame for easier handling\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=X_processed.columns, index=X_processed.index)\n",
    "\n",
    "# Verify scaling\n",
    "print(f\"\\n🔍 Scaling verification:\")\n",
    "print(f\"  • Mean of first 5 features: {X_scaled_df.iloc[:, :5].mean().round(3).tolist()}\")\n",
    "print(f\"  • Std of first 5 features: {X_scaled_df.iloc[:, :5].std().round(3).tolist()}\")\n",
    "\n",
    "# Final dataset summary\n",
    "print(f\"\\n📋 Final Dataset Summary:\")\n",
    "print(f\"  • Total samples: {X_scaled.shape[0]:,}\")\n",
    "print(f\"  • Total features: {X_scaled.shape[1]:,}\")\n",
    "print(f\"  • Target classes: {len(target_classes)} (3-class classification)\")\n",
    "print(f\"  • Class distribution: {dict(zip(target_classes, np.bincount(y_encoded)))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a94405",
   "metadata": {},
   "source": [
    "## 🤖 Model Training Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4897471c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Training Pipeline\n",
    "print(\"🚂 Kepler Model Training Pipeline:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_scaled, y_encoded, test_size=0.2, random_state=42, stratify=y_encoded\n",
    ")\n",
    "\n",
    "print(f\"📊 Data split:\")\n",
    "print(f\"  • Training set: {X_train.shape}\")\n",
    "print(f\"  • Test set: {X_test.shape}\")\n",
    "\n",
    "# Check class distribution in splits\n",
    "train_dist = np.bincount(y_train)\n",
    "test_dist = np.bincount(y_test)\n",
    "\n",
    "print(f\"\\n🎯 Training set distribution:\")\n",
    "for i, class_name in enumerate(target_classes):\n",
    "    print(f\"  • {class_name}: {train_dist[i]:,} ({train_dist[i]/len(y_train):.1%})\")\n",
    "\n",
    "print(f\"\\n🎯 Test set distribution:\")\n",
    "for i, class_name in enumerate(target_classes):\n",
    "    print(f\"  • {class_name}: {test_dist[i]:,} ({test_dist[i]/len(y_test):.1%})\")\n",
    "\n",
    "# Initialize models (consistent with TOI and K2 analysis)\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'Extra Trees': ExtraTreesClassifier(n_estimators=100, random_state=42),\n",
    "    'XGBoost': xgb.XGBClassifier(random_state=42, eval_metric='mlogloss'),\n",
    "    'SVM': SVC(random_state=42, probability=True)\n",
    "}\n",
    "\n",
    "print(f\"\\n🤖 Models to train: {list(models.keys())}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a50505ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate all models\n",
    "print(\"🎯 Training 5 models...\")\n",
    "print()\n",
    "\n",
    "results = {}\n",
    "trained_models = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"🔄 Training {name}...\")\n",
    "    \n",
    "    # Train the model\n",
    "    model.fit(X_train, y_train)\n",
    "    trained_models[name] = model\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Cross-validation\n",
    "    cv_scores = cross_val_score(model, X_train, y_train, cv=5, scoring='accuracy', \n",
    "                               cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42))\n",
    "    \n",
    "    results[name] = {\n",
    "        'accuracy': accuracy,\n",
    "        'cv_mean': cv_scores.mean(),\n",
    "        'cv_std': cv_scores.std(),\n",
    "        'predictions': y_pred\n",
    "    }\n",
    "    \n",
    "    print(f\"✅ {name} - Accuracy: {accuracy:.4f} | CV: {cv_scores.mean():.4f} (±{cv_scores.std():.4f})\")\n",
    "\n",
    "print(f\"\\n🏆 Model Training Completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0b6899",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance comparison and ranking\n",
    "print(\"🏆 Model Performance Ranking:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Create results DataFrame\n",
    "results_df = pd.DataFrame({\n",
    "    'Model': results.keys(),\n",
    "    'Test_Accuracy': [results[name]['accuracy'] for name in results.keys()],\n",
    "    'CV_Mean': [results[name]['cv_mean'] for name in results.keys()],\n",
    "    'CV_Std': [results[name]['cv_std'] for name in results.keys()]\n",
    "}).sort_values('Test_Accuracy', ascending=False)\n",
    "\n",
    "print(results_df.to_string(index=False, float_format='%.6f'))\n",
    "\n",
    "# Identify best model\n",
    "best_model_name = results_df.iloc[0]['Model']\n",
    "best_model = trained_models[best_model_name]\n",
    "best_predictions = results[best_model_name]['predictions']\n",
    "\n",
    "print(f\"\\n🥇 Best Model: {best_model_name}\")\n",
    "print(f\"  • Test Accuracy: {results[best_model_name]['accuracy']:.4f}\")\n",
    "print(f\"  • Cross-validation: {results[best_model_name]['cv_mean']:.4f} (±{results[best_model_name]['cv_std']:.4f})\")\n",
    "\n",
    "# Detailed classification report for best model\n",
    "print(f\"\\n📋 Classification Report - {best_model_name}:\")\n",
    "print(classification_report(y_test, best_predictions, target_names=target_classes))\n",
    "\n",
    "# Confusion matrix\n",
    "print(f\"\\n🔢 Confusion Matrix:\")\n",
    "cm = confusion_matrix(y_test, best_predictions)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0c7bea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization of results\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Plot 1: Model performance comparison\n",
    "ax1 = axes[0]\n",
    "models_list = results_df['Model'].tolist()\n",
    "accuracies = results_df['Test_Accuracy'].tolist()\n",
    "cv_means = results_df['CV_Mean'].tolist()\n",
    "\n",
    "x_pos = np.arange(len(models_list))\n",
    "ax1.bar(x_pos, accuracies, alpha=0.7, label='Test Accuracy', color='skyblue', edgecolor='black')\n",
    "ax1.plot(x_pos, cv_means, 'ro-', label='CV Mean', linewidth=2, markersize=6)\n",
    "ax1.set_xlabel('Models')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.set_title('Kepler Dataset - Model Performance', fontweight='bold')\n",
    "ax1.set_xticks(x_pos)\n",
    "ax1.set_xticklabels(models_list, rotation=45, ha='right')\n",
    "ax1.legend()\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "ax1.set_ylim(0.9, 1.0)\n",
    "\n",
    "# Plot 2: Cross-validation performance\n",
    "ax2 = axes[1]\n",
    "cv_means_plot = results_df['CV_Mean'].tolist()\n",
    "cv_stds_plot = results_df['CV_Std'].tolist()\n",
    "\n",
    "ax2.bar(x_pos, cv_means_plot, yerr=cv_stds_plot, alpha=0.7, \n",
    "        color='lightgreen', edgecolor='black', capsize=5)\n",
    "ax2.set_xlabel('Models')\n",
    "ax2.set_ylabel('CV Accuracy')\n",
    "ax2.set_title('Cross-Validation Performance', fontweight='bold')\n",
    "ax2.set_xticks(x_pos)\n",
    "ax2.set_xticklabels(models_list, rotation=45, ha='right')\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Plot 3: Confusion matrix heatmap\n",
    "ax3 = axes[2]\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=target_classes, yticklabels=target_classes, ax=ax3)\n",
    "ax3.set_title(f'Confusion Matrix - {best_model_name}', fontweight='bold')\n",
    "ax3.set_xlabel('Predicted')\n",
    "ax3.set_ylabel('Actual')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"📊 Model evaluation visualizations completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e1261a",
   "metadata": {},
   "source": [
    "## 📊 Feature Importance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9735c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance analysis for tree-based models\n",
    "print(\"📊 Feature Importance Analysis\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "tree_models = ['Random Forest', 'Extra Trees', 'XGBoost']\n",
    "feature_names = X_processed.columns\n",
    "\n",
    "for model_name in tree_models:\n",
    "    if model_name in trained_models:\n",
    "        model = trained_models[model_name]\n",
    "        \n",
    "        if hasattr(model, 'feature_importances_'):\n",
    "            importance = model.feature_importances_\n",
    "            \n",
    "            # Create importance DataFrame\n",
    "            importance_df = pd.DataFrame({\n",
    "                'Feature': feature_names,\n",
    "                'Importance': importance\n",
    "            }).sort_values('Importance', ascending=False)\n",
    "            \n",
    "            print(f\"\\n🌲 {model_name} - Top 10 Important Features:\")\n",
    "            print(importance_df.head(10).to_string(index=False))\n",
    "            \n",
    "            # Plot feature importance for best model\n",
    "            if model_name == best_model_name:\n",
    "                plt.figure(figsize=(12, 8))\n",
    "                top_features = importance_df.head(15)\n",
    "                plt.barh(range(len(top_features)), top_features['Importance'], \n",
    "                        color='steelblue', alpha=0.8, edgecolor='black')\n",
    "                plt.yticks(range(len(top_features)), top_features['Feature'])\n",
    "                plt.xlabel('Feature Importance')\n",
    "                plt.title(f'Top 15 Feature Importances - {best_model_name}\\n(Kepler 3-Class Classification)', \n",
    "                         fontweight='bold', fontsize=14)\n",
    "                plt.gca().invert_yaxis()\n",
    "                plt.grid(axis='x', alpha=0.3)\n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "\n",
    "print(f\"\\n✅ Feature importance analysis completed for tree-based models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63ed44d9",
   "metadata": {},
   "source": [
    "## 💾 Results Summary and Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adbd76ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kepler Analysis Final Summary and Integration Preparation\n",
    "print(\"🎯 Kepler Dataset Analysis - Final Summary\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "print(f\"📊 Dataset Statistics:\")\n",
    "print(f\"  • Total samples: {len(kepler_data):,}\")\n",
    "print(f\"  • Original features: {len(kepler_data.columns)}\")\n",
    "print(f\"  • Processed features: {X_scaled.shape[1]}\")\n",
    "print(f\"  • Target variable: koi_disposition (3-class classification)\")\n",
    "\n",
    "print(f\"\\n🏆 Model Performance (3-Class Classification):\")\n",
    "for i, (_, row) in enumerate(results_df.iterrows(), 1):\n",
    "    print(f\"  {i}. {row['Model']}: {row['Test_Accuracy']:.4f} accuracy\")\n",
    "\n",
    "print(f\"\\n🥇 Best Performing Model:\")\n",
    "print(f\"  • Model: {best_model_name}\")\n",
    "print(f\"  • Test Accuracy: {results[best_model_name]['accuracy']:.4f}\")\n",
    "print(f\"  • Cross-validation: {results[best_model_name]['cv_mean']:.4f} ± {results[best_model_name]['cv_std']:.4f}\")\n",
    "\n",
    "print(f\"\\n🎯 Target Classes (3-Class Problem):\")\n",
    "for class_name, count in target_counts.items():\n",
    "    percentage = (count / target_counts.sum()) * 100\n",
    "    print(f\"  • {class_name}: {count:,} samples ({percentage:.1f}%)\")\n",
    "\n",
    "# Feature importance for best model\n",
    "if best_model_name in tree_models and hasattr(trained_models[best_model_name], 'feature_importances_'):\n",
    "    model = trained_models[best_model_name]\n",
    "    importance = model.feature_importances_\n",
    "    \n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_names,\n",
    "        'Importance': importance\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    print(f\"\\n🌲 {best_model_name} - Top 10 Important Features:\")\n",
    "    print(importance_df.head(10).to_string(index=False))\n",
    "\n",
    "# Save results for integration\n",
    "print(f\"\\n💾 Saving Kepler Results for Multi-Dataset Integration:\")\n",
    "kepler_output_data = {\n",
    "    'processed_features': X_processed,\n",
    "    'scaled_features': X_scaled,\n",
    "    'target': y_encoded,\n",
    "    'target_names': target_classes,\n",
    "    'feature_names': feature_names.tolist(),\n",
    "    'best_model': best_model,\n",
    "    'best_model_name': best_model_name,\n",
    "    'results_summary': results_df,\n",
    "    'dataset_info': {\n",
    "        'name': 'Kepler',\n",
    "        'samples': len(kepler_data),\n",
    "        'original_features': len(kepler_data.columns),\n",
    "        'processed_features': X_scaled.shape[1],\n",
    "        'target_variable': 'koi_disposition',\n",
    "        'classes': 3,\n",
    "        'class_distribution': dict(zip(target_classes, np.bincount(y_encoded)))\n",
    "    },\n",
    "    'preprocessing_info': {\n",
    "        'scaler': scaler,\n",
    "        'target_encoder': target_encoder,\n",
    "        'label_encoders': label_encoders,\n",
    "        'numerical_imputer': numerical_imputer,\n",
    "        'categorical_imputer': SimpleImputer(strategy='most_frequent') if categorical_columns else None\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save results\n",
    "import pickle\n",
    "with open('kepler_analysis_corrected_results.pkl', 'wb') as f:\n",
    "    pickle.dump(kepler_output_data, f)\n",
    "\n",
    "print(\"✅ Results saved to 'kepler_analysis_corrected_results.pkl'\")\n",
    "\n",
    "print(f\"\\n🔬 Key Insights:\")\n",
    "print(\"  • Kepler dataset successfully analyzed using 3-class classification\")\n",
    "print(\"  • Target: koi_disposition (includes CONFIRMED exoplanets)\")\n",
    "print(\"  • Consistent methodology with TOI and K2 datasets\")\n",
    "print(\"  • Results ready for multi-dataset comparison and integration\")\n",
    "\n",
    "print(f\"\\n🚀 Integration Readiness:\")\n",
    "print(\"  • ✅ Kepler dataset analysis completed (3-class)\")\n",
    "print(\"  • ✅ TOI dataset analysis completed\") \n",
    "print(\"  • ✅ K2 dataset analysis completed\")\n",
    "print(\"  • 🔄 Ready for cross-dataset comparison\")\n",
    "print(\"  • 🔄 Ready for unified preprocessing pipeline\")\n",
    "print(\"  • 🔄 Ready for merged multi-mission training\")\n",
    "\n",
    "print(f\"\\n📋 Next Steps:\")\n",
    "print(\"  1. Update cross-dataset comparison with corrected Kepler results\")\n",
    "print(\"  2. Unified preprocessing pipeline development\")\n",
    "print(\"  3. Multi-dataset integration and training\")\n",
    "print(\"  4. Final NASA Space Apps challenge solution\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"✨ Kepler 3-Class Analysis Pipeline Completed Successfully! ✨\")\n",
    "print(\"🌟 Ready for Multi-Dataset Integration Phase! 🌟\")\n",
    "print(\"=\" * 50)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
